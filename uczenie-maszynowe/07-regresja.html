<!doctype html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>07-regresja</title>
    <link rel="stylesheet" href="../style.css">

    <!--    Load mathjax from cdn to render latex equations-->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="prev-next-links">
    
    <div class="index-links-prev">
        <a href="06-naiwny-klasyfikator-bayesowski.html">Poprzedni: 06-naiwny-klasyfikator-bayesowski.html</a>
    </div>
    

    
    <div class="index-links-next">
        <a href="08-ocena-jakosci.html">Następny: 08-ocena-jakosci.html</a>
    </div>
    
    <div class="return-link">
        <a href="..">Powrót</a>
    </div>
</div>
<div class="container">
    <div class="index-links-wrapper">
        <h2>Uczenie maszynowe</h2>
        <div class="index-links">
            <ul>
                
                <li><a href="00-organizacja.html">00-organizacja.html</a></li>
                
                <li><a href="01-indukcyjne-uczenie-sie.html">01-indukcyjne-uczenie-sie.html</a></li>
                
                <li><a href="02-teoria-uczenia-sie.html">02-teoria-uczenia-sie.html</a></li>
                
                <li><a href="03-uczenie-sie-przestrzeni-wersji.html">03-uczenie-sie-przestrzeni-wersji.html</a></li>
                
                <li><a href="04-indukcja-regul.html">04-indukcja-regul.html</a></li>
                
                <li><a href="05-drzewa-decyzyjne.html">05-drzewa-decyzyjne.html</a></li>
                
                <li><a href="06-naiwny-klasyfikator-bayesowski.html">06-naiwny-klasyfikator-bayesowski.html</a></li>
                
                <li><a href="07-regresja.html">07-regresja.html</a></li>
                
                <li><a href="08-ocena-jakosci.html">08-ocena-jakosci.html</a></li>
                
                <li><a href="09-las-losowy.html">09-las-losowy.html</a></li>
                
                <li><a href="10-svm.html">10-svm.html</a></li>
                
                <li><a href="11-uczenie-sie-ze-wzmocnieniem.html">11-uczenie-sie-ze-wzmocnieniem.html</a></li>
                
            </ul>
        </div>
    </div>
    <div class="content-wrapper">
        <main>
            <h1 id="regresja">Regresja</h1>
<h2 id="regresja-liniowa">Regresja liniowa</h2>
<h3 id="model-liniowy">Model liniowy</h3>
<p><span class="math display">\[
h(x)
= \sum_{i=1}^n w_i a_i(x) + w_{n+1}
= \sum_{i=1}^{n+1}w_i a_i(x)
= \mathbf{w} \cdot \mathbf{a}(x)
\]</span></p>
<ul>
<li>Kombinacja liniowa</li>
<li>Wektor wag <span class="math inline">\(\mathbf{w}\)</span></li>
<li>Wektor atrybutów <span class="math inline">\(\mathbf{a}(x)\)</span>
(ciągłych)</li>
<li>Wyraz wolny oznaczamy jako <span
class="math inline">\(w_{n+1}\)</span></li>
<li>Zapis w postaci iloczynu skalarnego, zakładamy, że <span
class="math inline">\(a_{n+1}(x) = 1\)</span></li>
<li>Estymacja parametrów
<ul>
<li>minimalizacja <em>funkcji straty</em> proporcjonalnej do błędu
średniokwadratowego na zbiorze trenującym</li>
<li><span class="math inline">\(E_{T,f}(h) = \frac{1}{2} \sum_{x\in
T}(f(x)-h(x))^2\)</span></li>
<li>nie ma dzielenia przez liczbę przykładów - dla wygody zapisu, nie
robi różnicy</li>
</ul></li>
<li>Cały model jest reprezentowany przez wektor parametrów
<ul>
<li>uczenie polega na doborze (dopasowaniu) parametrów</li>
</ul></li>
</ul>
<h3 id="metoda-gradientowa">Metoda gradientowa</h3>
<p><span class="math display">\[ w := w + \beta (-\nabla_w E_{T,f}(h))
\]</span> <span class="math display">\[ \nabla_wE_{T,f}(h) = 1/2 \sum_{x
\in T} 2(f(x)-h(x)) \nabla_w(-h(x)) \]</span> <span
class="math display">\[ = \sum_{x \in T} (f(x)-h(x))(-a(x)) \]</span> *
Stosowane iteracyjnie * Wymaga wyliczenia gradientu na całym zbiorze
trenującym * Inicjalizacja wag * same 1 * małe wartości losowe *
Bardziej zaawansowane metody gradientowe * metoda Newtona * Adam *
AdaGrad</p>
<h3 id="stochastyczny-spadek-gradientu">Stochastyczny spadek
gradientu</h3>
<ul>
<li>SGD</li>
<li>Aktualizacja dla pojedynczych przykładów w kolejności losowej</li>
<li>Poprawia jakość dla jednego przykładu na raz
<ul>
<li>wielokrotne powtarzanie daje efekt uśredniający</li>
</ul></li>
<li>Częściej używany
<ul>
<li>nie wymaga obliczeń na całym zbiorze trenującym na raz</li>
<li>zbiór trenujący nie musi być dostępny cały w pamięci, można po 1 na
raz</li>
</ul></li>
</ul>
<p><span class="math display">\[ w := w + \beta(f(x)-h(x))a(x)
\]</span></p>
<h3 id="metoda-najmniejszych-kwadratów">Metoda najmniejszych
kwadratów</h3>
<ul>
<li>Zamknięta formuła na wyznaczenie parametrów</li>
<li>Układ równań
<ul>
<li><span class="math inline">\(a_1(x_i)w_1 + \ldots + a_n(x_i)w_n +
a_{n+1}(x_i)w_{n+1} = f(x)\)</span></li>
<li>niewiadome to współczynniki modelu</li>
<li>tyle równań ile przykładów</li>
<li>liczba przykładów znacznie większa niż liczba atrybutów</li>
</ul></li>
<li>Zapis macierzowy <span class="math inline">\(Aw=f\)</span>
<ul>
<li>A - macierz wartości atrybutów</li>
<li><span class="math inline">\(|T|\)</span> wierszy</li>
<li><span class="math inline">\(n+1\)</span> kolumn</li>
<li><span class="math inline">\(A^TAw=A^Tf\)</span></li>
<li><span class="math inline">\(A^TA\)</span> - macierz <span
class="math inline">\((n+1) \times (n+1)\)</span></li>
<li><span class="math inline">\(w = (A^TA)^{-1}A^Tf\)</span></li>
</ul></li>
<li>Tą metodą oblicza się w praktyce parametry dla modeli liniowych</li>
</ul>
<h2 id="klasyfikacja-liniowo-progowa">Klasyfikacja liniowo-progowa</h2>
<h3 id="reprezentacja-liniowo-progowa">Reprezentacja
liniowo-progowa</h3>
<ul>
<li>Wewnętrzna reprezentacja liniowa <span class="math inline">\(g(x) =
\mathbf{w} \cdot \mathbf{a}(x)\)</span></li>
<li>Zewnętrzna funkcja progowa <span class="math inline">\(h(x) =
1\)</span> jeśli <span class="math inline">\(g(x) \ge 0\)</span>, <span
class="math inline">\(0\)</span> w przeciwnym przypadku
<ul>
<li>klasyfikacja binarna</li>
<li>granica decyzyjna</li>
</ul></li>
<li>Odrzuca się jeden wymiar z hiperpłaszczyzny</li>
<li>Wymiar VC <span class="math inline">\(n+1\)</span>
<ul>
<li>ograniczone ryzyko nadmiernego dopasowania</li>
</ul></li>
</ul>
<h3 id="odległość-od-granicy-decyzyjnej">Odległość od granicy
decyzyjnej</h3>
<p>Odległość ze znakiem (dla klasy 1 dodatnia, dla klasy 0 ujemna)</p>
<p><span class="math display">\[ \delta_w(x) = \frac{w \cdot
a(x)}{\|w_{1:n}\|} \]</span></p>
<p>Odległość bezwzględna dla przykładów niepoprawnie klasyfikownanych
(sztuczka) <span class="math inline">\(-c_-(x) \delta_w(x)\)</span></p>
<p><span class="math display">\[ c_-(x) = 2c(x) - 1 = \begin{cases}
    +1 \quad dla \quad c(x)=1 \\
    -1 \quad dla \quad c(x) = 0 \\
\end{cases}
\]</span> Funkcja decyzyjna <span class="math inline">\(g(x) = w \cdot
a(x)\)</span> podejmuje decyzje o przewidywanej klasie używając jakiejś
funkcji rzeczywistoliczbowej, odległość od granicy określa ufność
predykcji</p>
<h3 id="algorytm-prosty-perceptron">Algorytm prosty perceptron</h3>
<ul>
<li>Nieprzydatny</li>
<li>Pokazany dla odniesienia z innymi metodami</li>
<li>Inicjalizacja dowolna, np. same 0</li>
<li>Aktualizowane parametry jeśli predykcja jest niepoprawna
<ul>
<li><span class="math inline">\(w:= w + c_- a(x)\)</span></li>
<li>jak algorytm gradientowy</li>
</ul></li>
<li>Zmniejszanie odległości od granicy decyzyjnej dla przykładów źle
klasyfikowanych</li>
<li>Jeśli istnieje granica decyzyjna to algorytm ją znajdzie</li>
<li>Dla wszystkich przykładów łącznie lub przyrostowo dla pojedynczych
przykładów</li>
<li>Kryterium stopu - poprawna klasyfikacja wszystkich przykładów
<ul>
<li>daje się zastosować tylko dla liniowo serparowalnych klas</li>
</ul></li>
<li>Zbieżność
<ul>
<li>gwarantowana tylko jeśli w zbiorze trenującym klasy są liniowo
separowalne</li>
<li>dlatego niepraktyczny</li>
</ul></li>
</ul>
<h2 id="regresja-logistyczna">Regresja logistyczna</h2>
<p>Wewnętrzna reprezentacja liniowa <span class="math inline">\(g(x) = w
\cdot a(x)\)</span></p>
<p>Zewnętrzna logistyczna funkcja łącząca</p>
<p><span class="math display">\[ logit(p) = \ln \frac{p}{1-p} \]</span>
<span class="math display">\[ logit^{-1}(q) = \frac{e^q}{e^q+1} =
\frac{1}{1+e^{-q}} \]</span> <span class="math display">\[
logit^{-1}(g(x)) = \pi(x) = P(1|x) \]</span></p>
<p>Interpretacja</p>
<p><span class="math display">\[ logit(P(1|x)) = \ln
\frac{P(1|x)}{P(0|x)} \]</span></p>
<p>Chcemy powiązać wartości z prawdopodobieństwami klas</p>
<p>Jeśli wyjście modelu liniowego możemy potraktować jako logarytm z
ilorazu prawdopodobieństw to stosując funkcję odwrotną możemy odzyskać
prawdopodobieństwo klasy</p>
<h3 id="wyznaczanie-parametrów-regresji-logistycznej">Wyznaczanie
parametrów regresji logistycznej</h3>
<p>Logarytm wiarygodności (log-likelihood) - miara zgodności predykcji
prawdopodobieństw klas z prawdziwymi klasami</p>
<p><span class="math display">\[ LL_{T,c}(\pi) = \ln P(T,c; \pi) = \ln
\prod_{x \in T} P(c(x) | x; \pi) \]</span></p>
<p>Prawdopodobieństwo klas jakie mają przykłady trenująca w świetle tego
jaki jest model</p>
<p><span class="math display">\[P(c(x)|x;\pi) = \begin{cases}
P(1|x) = \pi(x), \quad c(x)=1 \\
P(0|x) = 1 - \pi(x), \quad c(x)=0
\end{cases}\]</span></p>
<p>Dobry model mówi, że prawdziwe klasy są bardzo prawdopodobne, a
nieprawdziwe mało prawdopodobne</p>
<p>Stosujemy sztuczkę, żeby zapisać wzór w jednej formule bez rozejść
warunkowych, traktujemy klasę jako wartość liczbową</p>
<p><span class="math display">\[ \ln \prod_{x \in T} \pi(x)^{c(x)}
(1-\pi(x))^{1-c(x)} \]</span> <span class="math display">\[ \sum_{x \in
T} (c(x) \ln \pi(x) + (1-c(x)) \ln (1-\pi(x))) \]</span></p>
<p>Oblicza się gradient z tego wzoru i stosuje do metody gradientowej,
taka sama postać jak gradient w regresji liniowej</p>
<p><span class="math display">\[ \nabla_w LL_{T,c}(\pi) = \sum_{x \in T}
(c(x) - \pi(x))a(x) \]</span> <span class="math display">\[ w := w +
\beta \nabla_2 LL_{T,c}(\pi) \]</span></p>
<ul>
<li>W efekcie uzyskuje się maksymalną zgodność predykcji z
prawdopodobieństwami prawdziwych klas</li>
<li>Radzi sobie z klasami, które nie są liniowo separowalne</li>
<li>Pojawia się w sieciach neuronowych</li>
</ul>
<h2 id="zagadnienia-praktyczne">Zagadnienia praktyczne</h2>
<h3 id="atrybuty-dyskretne">Atrybuty dyskretne</h3>
<ul>
<li>Kodowanie binarne - zastąpienie przez <span
class="math inline">\(k-1\)</span> atrybutów binarnych
<ul>
<li>one hot encoding</li>
<li>dummy variables</li>
<li>lepiej kodować przez o 1 mniej atrybutów - nie można reprezentować
nielegalnej wartości</li>
</ul></li>
<li>Kodowanie przez <span class="math inline">\(k\)</span> atrybutów
binarnych jest szkodliwe
<ul>
<li>liniowa zależność</li>
<li>gdzieś może powstać macierz nieosobliwa</li>
</ul></li>
</ul>
<h3 id="klasyfikacja-wieloklasowa">Klasyfikacja wieloklasowa</h3>
<ul>
<li>1 kontra reszta (one vs rest, OVR)
<ul>
<li>osobny model binarny dla każdej klasy</li>
<li>predykcja przez wybór klasy o maksymalnej wartości funkcji
decyzyjnej</li>
<li>może się zdarzyć, że więcej niż jeden model daje predykcję
pozytywną</li>
<li>trzeba użyć prawdopodobieństwa lub wartości funkcji decyzyjnej dla
rozstrzygnięcia</li>
</ul></li>
<li>1 kontra 1 (one vs one, OvO)
<ul>
<li>osobny model binarny dla każdej pary klas</li>
<li>predykcja przez głosowanie</li>
</ul></li>
</ul>
<h3 id="właściwośi-modeli-liniowych">Właściwośi modeli liniowych</h3>
<ul>
<li>Wyłącznie zalezności przynajmniej w przybliżeniu liniowe lub liniowo
separowalne</li>
<li>Dobór parametrów odporny na optima lokalne</li>
<li>Efektywna metoda najmniejszych kwadratów (dla regresji
liniowej)</li>
<li>Interpretowalne parametry modelu</li>
<li>Ograniczone ryzyko nadmiernego dopasowania</li>
<li>Możliwe bardziej zaawansowane warianty
<ul>
<li>częściowo lub cał…owicie pokonują ograniczenia liniowości</li>
<li>np. SVM/SVR</li>
</ul></li>
</ul>

        </main>
    </div>
    <div class="table-of-contents">
        <nav id="TOC" role="doc-toc">
<ul>
<li><a href="#regresja" id="toc-regresja">Regresja</a>
<ul>
<li><a href="#regresja-liniowa" id="toc-regresja-liniowa">Regresja
liniowa</a>
<ul>
<li><a href="#model-liniowy" id="toc-model-liniowy">Model
liniowy</a></li>
<li><a href="#metoda-gradientowa" id="toc-metoda-gradientowa">Metoda
gradientowa</a></li>
<li><a href="#stochastyczny-spadek-gradientu" id="toc-stochastyczny-spadek-gradientu">Stochastyczny spadek
gradientu</a></li>
<li><a href="#metoda-najmniejszych-kwadratów" id="toc-metoda-najmniejszych-kwadratów">Metoda najmniejszych
kwadratów</a></li>
</ul></li>
<li><a href="#klasyfikacja-liniowo-progowa" id="toc-klasyfikacja-liniowo-progowa">Klasyfikacja liniowo-progowa</a>
<ul>
<li><a href="#reprezentacja-liniowo-progowa" id="toc-reprezentacja-liniowo-progowa">Reprezentacja
liniowo-progowa</a></li>
<li><a href="#odległość-od-granicy-decyzyjnej" id="toc-odległość-od-granicy-decyzyjnej">Odległość od granicy
decyzyjnej</a></li>
<li><a href="#algorytm-prosty-perceptron" id="toc-algorytm-prosty-perceptron">Algorytm prosty perceptron</a></li>
</ul></li>
<li><a href="#regresja-logistyczna" id="toc-regresja-logistyczna">Regresja logistyczna</a>
<ul>
<li><a href="#wyznaczanie-parametrów-regresji-logistycznej" id="toc-wyznaczanie-parametrów-regresji-logistycznej">Wyznaczanie
parametrów regresji logistycznej</a></li>
</ul></li>
<li><a href="#zagadnienia-praktyczne" id="toc-zagadnienia-praktyczne">Zagadnienia praktyczne</a>
<ul>
<li><a href="#atrybuty-dyskretne" id="toc-atrybuty-dyskretne">Atrybuty
dyskretne</a></li>
<li><a href="#klasyfikacja-wieloklasowa" id="toc-klasyfikacja-wieloklasowa">Klasyfikacja wieloklasowa</a></li>
<li><a href="#właściwośi-modeli-liniowych" id="toc-właściwośi-modeli-liniowych">Właściwośi modeli
liniowych</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
    </div>
</div>
</body>
</html>