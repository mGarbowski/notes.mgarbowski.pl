<!doctype html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>00-organizacja</title>
    <link rel="stylesheet" href="../style.css">

    <!--    Load mathjax from cdn to render latex equations-->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="prev-next-links">
    

    
    <div class="index-links-next">
        <a href="01-wstep.html">Następny: 01-wstep.html</a>
    </div>
    
    <div class="return-link">
        <a href="..">Powrót</a>
    </div>
</div>
<div class="container">
    <div class="index-links-wrapper">
        <h2>Podstawy wielkich modeli językowych</h2>
        <div class="index-links">
            <ul>
                
                <li><a href="00-organizacja.html">00-organizacja.html</a></li>
                
                <li><a href="01-wstep.html">01-wstep.html</a></li>
                
                <li><a href="02-podstawy-uczenia-glebokiego.html">02-podstawy-uczenia-glebokiego.html</a></li>
                
                <li><a href="03-elementy-skladowe-sieci-neuronowych.html">03-elementy-skladowe-sieci-neuronowych.html</a></li>
                
                <li><a href="04-tokenizacja.html">04-tokenizacja.html</a></li>
                
                <li><a href="05-transformer.html">05-transformer.html</a></li>
                
                <li><a href="06-tylko-dekoder.html">06-tylko-dekoder.html</a></li>
                
                <li><a href="07-fezy-trenowania.html">07-fezy-trenowania.html</a></li>
                
                <li><a href="08-efektywne-obliczeniowo-dostrajanie-modeli.html">08-efektywne-obliczeniowo-dostrajanie-modeli.html</a></li>
                
                <li><a href="09-tylko-koder.html">09-tylko-koder.html</a></li>
                
                <li><a href="10-modele-wizyjno-jezykowe.html">10-modele-wizyjno-jezykowe.html</a></li>
                
                <li><a href="11-retrieval-augmented-generation.html">11-retrieval-augmented-generation.html</a></li>
                
                <li><a href="kolos.html">kolos.html</a></li>
                
            </ul>
        </div>
    </div>
    <div class="content-wrapper">
        <main>
            <h1 id="organizacja">Organizacja</h1>
<ul>
<li>dr Jacek Komorowski</li>
<li>Materiały na kanale teams
<ul>
<li>wykłady</li>
<li>tutoriale z torcha</li>
</ul></li>
<li>Używamy pytorch i huggingface</li>
<li>Wykład
<ul>
<li>10 pkt</li>
<li>jedno kolokwium</li>
<li>ostatni miesiąc zajęć</li>
</ul></li>
<li>Laboratorium
<ul>
<li>14 pkt</li>
<li>7x 2pkt</li>
<li>2 tygodnie na przesłanie rozwiązania</li>
<li>start 10 X (piątek) i 14 X (wtorek)</li>
</ul></li>
<li>Projekt
<ul>
<li>14 pkt</li>
<li>samodzielny lub zespoły 2-osobowe</li>
<li>tydzień na potwierdzenie tematu projektu i wykorzystywanych zbiorów
danych</li>
<li>kod + raport + prezentacja wyników</li>
<li>na początku listopada więcej informacji</li>
<li>będą proponowane tematy</li>
</ul></li>
<li>Zaliczenie
<ul>
<li>min. 50% z każdej części</li>
</ul></li>
</ul>
<p>Będziemy też omawiać klasyczne architektury i metody - bywają
szybsze, lepsze i tańsze niż LLM</p>
<h2 id="plan-wykładu">Plan wykładu</h2>
<ul>
<li>Wprowadzenie i podstawy
<ul>
<li>podstawy głębokiego uczenia i głębokich sieci neuronowych</li>
<li>pytorch</li>
<li>klasyczne metody reprezentacji i klasyfikacji tekstu</li>
</ul></li>
<li>Budowa wielkich modeli językowych
<ul>
<li>reprezentacja tekstów w języku naturalnym</li>
<li>metody tokenizacji</li>
<li>architektura transformer i jej składowe</li>
<li>rodziny architektur modeli językowych (tylko-koder, tylko-dekoder,
…)</li>
</ul></li>
<li>Uczenie wielkich modeli językowych</li>
<li>Wykorzystanie i dostosowywanie modeli językowych
<ul>
<li>strategie generowania tekstu</li>
<li>wykorzystanie modeli do zadań NLP</li>
<li>metody efektywnego obliczeniowo dostrajania (PEFT)</li>
</ul></li>
<li>Modele wizyjno-językowe
<ul>
<li>uczenie reprezentacji wizyjno-językowej (CLIP)</li>
<li>generatywne modele wizyjno-językowe</li>
</ul></li>
<li>Zastosowania
<ul>
<li>generowanie tekstu wspomagane wyszukiwaniem (RAG)</li>
<li>systemy dialogowe</li>
<li>agenty językowe</li>
</ul></li>
<li>Polskie modele językowe</li>
<li>Współczesne trendy rozwojowe</li>
</ul>
<h2 id="literatura">Literatura</h2>
<ul>
<li>Książki dostępne online, linki na slajdach</li>
<li>Jurafsky
<ul>
<li>https://web.stanford.edu/~jurafsky/slp3/</li>
</ul></li>
<li>Książki
<ul>
<li>https://thelmbook.com/#chapters</li>
</ul></li>
<li>PyTorch
<ul>
<li>samouczki w dokumentacji</li>
</ul></li>
<li>HuggingFace
<ul>
<li>LLM Course https://huggingface.co/learn/llm-course/chapter1/1</li>
<li>dokumentacja pakietu Transformers</li>
</ul></li>
<li>Wykłady dostępne online
<ul>
<li>Stanford CS224N: NLP with Deep Learning</li>
<li>CMU Advanced NLP Spring 2025</li>
<li>Berkeley Agent AI MOOC</li>
</ul></li>
</ul>

        </main>
    </div>
    <div class="table-of-contents">
        <nav id="TOC" role="doc-toc">
<ul>
<li><a href="#organizacja" id="toc-organizacja">Organizacja</a>
<ul>
<li><a href="#plan-wykładu" id="toc-plan-wykładu">Plan wykładu</a></li>
<li><a href="#literatura" id="toc-literatura">Literatura</a></li>
</ul></li>
</ul>
</nav>
    </div>
</div>
</body>
</html>