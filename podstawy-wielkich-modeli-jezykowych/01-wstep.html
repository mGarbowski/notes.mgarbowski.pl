<!doctype html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>01-wstep</title>
    <link rel="stylesheet" href="../style.css">

    <!--    Load mathjax from cdn to render latex equations-->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="prev-next-links">
    
    <div class="index-links-prev">
        <a href="00-organizacja.html">Poprzedni: 00-organizacja.html</a>
    </div>
    

    
    <div class="index-links-next">
        <a href="02-podstawy-uczenia-glebokiego.html">Następny: 02-podstawy-uczenia-glebokiego.html</a>
    </div>
    
    <div class="return-link">
        <a href="..">Powrót</a>
    </div>
</div>
<div class="container">
    <div class="index-links-wrapper">
        <h2>Podstawy wielkich modeli językowych</h2>
        <div class="index-links">
            <ul>
                
                <li><a href="00-organizacja.html">00-organizacja.html</a></li>
                
                <li><a href="01-wstep.html">01-wstep.html</a></li>
                
                <li><a href="02-podstawy-uczenia-glebokiego.html">02-podstawy-uczenia-glebokiego.html</a></li>
                
                <li><a href="03-elementy-skladowe-sieci-neuronowych.html">03-elementy-skladowe-sieci-neuronowych.html</a></li>
                
                <li><a href="04-tokenizacja.html">04-tokenizacja.html</a></li>
                
                <li><a href="05-transformer.html">05-transformer.html</a></li>
                
                <li><a href="06-tylko-dekoder.html">06-tylko-dekoder.html</a></li>
                
                <li><a href="07-fezy-trenowania.html">07-fezy-trenowania.html</a></li>
                
                <li><a href="08-efektywne-obliczeniowo-dostrajanie-modeli.html">08-efektywne-obliczeniowo-dostrajanie-modeli.html</a></li>
                
                <li><a href="09-tylko-koder.html">09-tylko-koder.html</a></li>
                
                <li><a href="10-modele-wizyjno-jezykowe.html">10-modele-wizyjno-jezykowe.html</a></li>
                
                <li><a href="11-retrieval-augmented-generation.html">11-retrieval-augmented-generation.html</a></li>
                
                <li><a href="kolos.html">kolos.html</a></li>
                
            </ul>
        </div>
    </div>
    <div class="content-wrapper">
        <main>
            <h1 id="wstęp">Wstęp</h1>
<h2 id="czym-są-wielkie-modele-językowe">Czym są wielkie modele
językowe</h2>
<ul>
<li>Generatywne modele językowe oparte o architekturę Transformer z
rodziny tylko-dekoder
<ul>
<li>transformator cech głębokich z atencją</li>
</ul></li>
<li>Trenowane na ogromnych zbiorach danych
<ul>
<li>setki miliardów lub biliony słów</li>
<li>główne zadanie - przewidywanie kolejnego słowa/tokenu na podstawie
kontekstu</li>
<li>uczą się statystycznych wzorców języka</li>
</ul></li>
<li>Dzięki skalowaniu parametrów i danych wykazują zdolności emergentne
(<em>emergent abilities</em>)
<ul>
<li>zdolność do wykonywania zadań, do których nie był specjalnie
trenowany</li>
<li>rozumowanie, pisanie kodu, streszczanie, tłumaczenie</li>
</ul></li>
<li>Liczba parametrów - rzędu dziesiątek miliardów lub więcej</li>
</ul>
<h3 id="dwie-perspektywy">Dwie perspektywy</h3>
<ul>
<li>LLM jako klasyfikator probabilistyczny
<ul>
<li>estymacja prawdopodobieństwa kolejnego tokenu na podstawie kontekstu
(poprzedzającej sekwencji)</li>
<li>wejście: kontekst (poprzedzająca sekwencja tokenów)</li>
<li>wyjście: rozkład prawdopodobieństwa kolejnego tokenu</li>
<li>funkcja straty: entropia krzyżowa (w fazie wstępnego trenowania i
nadzorowanego dostrajania)</li>
</ul></li>
<li>LLM jako statystyczny model języka
<ul>
<li>podobnie jak tradycyjne modele językowe (n-gramowe, HMM, RNN)</li>
<li>definiuje rozkład prawdopodobieństwa na sekwencjach
słów/tokenów</li>
<li>prawdopodobieństwo tokenu <span class="math inline">\(x_n\)</span>
warunkowane poprzedzającą sekwencją <span class="math inline">\(x_1,
\ldots, x_{n-1}\)</span></li>
</ul></li>
</ul>
<h2 id="systemy-dialogowe">Systemy dialogowe</h2>
<ul>
<li>System dialogowy zapamiętuje historię konwersacji
<ul>
<li>LLM jest komponentem systemu dialogowego</li>
<li>kontekst podawany na wejście LLM</li>
</ul></li>
<li>Prompt systemowy
<ul>
<li>“jesteś asystentem AI …”</li>
<li>podawany na wejście LLM przed promptem użytkownika</li>
</ul></li>
<li>System dialogowy może wywoływać dodatkowe narzędzia
<ul>
<li>LLM może wygenerować polecenie zrozumiałe dla systemu
dialogowego</li>
<li>np. specjalny token</li>
<li>odpowiedź z narzędzia też jest doklejona do sekwencji wejściowej
modelu</li>
</ul></li>
</ul>
<h2 id="autoregresyjne-generowanie-tekstu">Autoregresyjne generowanie
tekstu</h2>
<ul>
<li>Podejście autoregresyjne przyczynkowe (causal)
<ul>
<li>generuje rozkład prawdopodobieństwa kolejnego tokenu</li>
<li>próbkowanie kolejnego tokenu (różne strategie) z rozkładu</li>
<li>doklejenie tokenu do sekwencji</li>
<li>powtórzenie aż do wygenerowania specjalnego tokenu end-of-sequence
(lub przekroczenie limitu długości)</li>
</ul></li>
<li>Problem długiego ogona
<ul>
<li>kiedy wybiera się zawsze najbardziej prawdopodobny token</li>
<li>powstaje długi, poprawny składniowo i bezsensowny tekst</li>
</ul></li>
</ul>
<h2 id="trenowanie-wstępne-pretraining">Trenowanie wstępne
(<em>pretraining</em>)</h2>
<ul>
<li>Na to poświęca się większość budżetu i zasobów obliczeniowych</li>
<li>Zadanie - przewidywanie kolejnego tokenu na podstawie kontekstu</li>
<li>Żeby model dobrze przewidywał prawdopodobieństwo kolejnego tokenu to
musi nauczyć się
<ul>
<li>wiedzy o świecie</li>
<li>reguł języka</li>
<li>schematów rozumowania i wnioskowania</li>
</ul></li>
<li>Olbrzymie zbiory danych
<ul>
<li>dane pozyskane z internetu (legalnie lub nie) - książki, repozytoria
kodu, serwisy społecznościowe</li>
<li>często używa się syntetycznie generowane dane - np. trenowanie
mniejszego modelu na podstawie generacji większego modelu</li>
<li>komercyjne LLM mające 7-70B parametrów - trening na zbiorze rzędu
1-15T tokenów (5-20TB tekstu)</li>
</ul></li>
<li>Koszty
<ul>
<li>GPT-3 - 5M USD</li>
<li>GPT-4 - 50-100M USD</li>
</ul></li>
</ul>
<h2 id="rodziny-architektur-modeli-językowych">Rodziny architektur
modeli językowych</h2>
<ul>
<li>Tylko-dekoder
<ul>
<li>np. GPT, Claude, LLaMA, Mistral</li>
<li>iteracyjne generowanie kolejnych tokenów</li>
<li>predykcja kolejnego tokenu</li>
<li>atencja przyczynowa - przepływ informacji od lewej do prawej</li>
<li><span class="math inline">\(x_i\)</span> nie korzysta z <span
class="math inline">\(x_{i+1}\)</span></li>
</ul></li>
<li>Tylko-koder
<ul>
<li>np. BERT, RoBERTA</li>
<li>wyznacza kontekstową reprezentację tokenów w sekwencji</li>
<li>atencja dwukierunkowa</li>
<li>trening - maskowane modelowanie języka</li>
<li>zwykle znacznie mniejsze rozmiary (rzędu 100M parametrów)</li>
<li>ekstrakcja reprezentacji, klasyfikacja tekstu</li>
<li>zastosowania typu RAG</li>
</ul></li>
<li>Koder-dekoder
<ul>
<li>transformer z artykułu <em>Attention is all you need</em></li>
<li>predykcja kolejnego tokenu na podstawie kontekstu, iteracyjne
generowanie kolejnych tokenów</li>
<li>koder - atencja dwukierunkowa</li>
<li>dekoder - atencja przyczynowa</li>
<li>tokeny wejściowe mogą być innego typu niż wyjściowe</li>
<li>np. audio -&gt; transkrypcja, obraz -&gt; opis tekstowy, wideo -&gt;
tekstowe podsumowanie</li>
</ul></li>
</ul>
<h3 id="tylko-dekoder">Tylko-dekoder</h3>
<ul>
<li>Tekst wejściowy - po tokenizacji
<ul>
<li>wielkość słownika np. 128k tokenów</li>
</ul></li>
<li>Identyfikatory tokenów
<ul>
<li>indeks w LUT</li>
</ul></li>
<li>Wektorowe reprezentacje tokenów
<ul>
<li>rozmiar wektora np. 4096</li>
<li>wybór wektora po indeksie</li>
<li>wagi inicjowane losowo</li>
<li>aktualizowane w trakcie treningu</li>
</ul></li>
<li>Position embedding
<ul>
<li>dodanie do embeddingu wektora reprezentującego jego pozycję w
sekwencji</li>
<li>różne podejścia, np. sinusowe</li>
<li>sama architektura nie korzysta gdzie indziej z informacji o
pozycji</li>
</ul></li>
<li>Wiele bloków dekodera
<ul>
<li>losowo inicjowane wagi, uczone</li>
<li>np. 32 warstwy</li>
<li>warstwa atencji</li>
<li>warstwa MLP</li>
<li>format wyjścia bloku taki sam jak wejścia - tensor (długość
sekwencji/kontekstu, rozmiar wektora zanurzenia)</li>
</ul></li>
<li>Kontekstowe reprezentacje tokenów
<ul>
<li>bierze się ostatni wektor cech z sekwencji</li>
<li>warstwa liniowa - rozmiar zanurzenia x rozmiar słownika</li>
<li>wyznacza indeks tokenu - da się zamienić z powrotem na tekst</li>
</ul></li>
<li>Liczba wag modelu nie zależy od długości sekwencji</li>
<li>Równoległa predykcja dla każdego tokenu w sekwencji wejściowej
<ul>
<li>tylko przy treningu!</li>
<li>dla każdego wyjścia można policzyć stratę</li>
<li>do tego atencja musi być przyczynowa - żeby model przy treningu nie
podglądał co jest dalej w sekwencji</li>
</ul></li>
</ul>

        </main>
    </div>
    <div class="table-of-contents">
        <nav id="TOC" role="doc-toc">
<ul>
<li><a href="#wstęp" id="toc-wstęp">Wstęp</a>
<ul>
<li><a href="#czym-są-wielkie-modele-językowe" id="toc-czym-są-wielkie-modele-językowe">Czym są wielkie modele
językowe</a>
<ul>
<li><a href="#dwie-perspektywy" id="toc-dwie-perspektywy">Dwie
perspektywy</a></li>
</ul></li>
<li><a href="#systemy-dialogowe" id="toc-systemy-dialogowe">Systemy
dialogowe</a></li>
<li><a href="#autoregresyjne-generowanie-tekstu" id="toc-autoregresyjne-generowanie-tekstu">Autoregresyjne generowanie
tekstu</a></li>
<li><a href="#trenowanie-wstępne-pretraining" id="toc-trenowanie-wstępne-pretraining">Trenowanie wstępne
(<em>pretraining</em>)</a></li>
<li><a href="#rodziny-architektur-modeli-językowych" id="toc-rodziny-architektur-modeli-językowych">Rodziny architektur
modeli językowych</a>
<ul>
<li><a href="#tylko-dekoder" id="toc-tylko-dekoder">Tylko-dekoder</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
    </div>
</div>
</body>
</html>